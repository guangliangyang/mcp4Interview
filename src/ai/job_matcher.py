"""
èŒä½åŒ¹é…åº¦åˆ†æå™¨
åˆ†æç®€å†ä¸èŒä½çš„åŒ¹é…ç¨‹åº¦ï¼Œæä¾›ä¼˜åŒ–å»ºè®®
"""

import asyncio
import logging
import re
from typing import Dict, List, Optional, Tuple
from pathlib import Path

from src.utils.logger import get_logger

logger = get_logger(__name__)

class JobMatcher:
    """èŒä½åŒ¹é…åº¦åˆ†æå™¨"""

    def __init__(self, ai_config: Dict):
        self.config = ai_config
        self.client = self._initialize_ai_client()

        # æŠ€èƒ½æƒé‡é…ç½®
        self.skill_weights = {
            'programming_languages': 0.3,
            'frameworks': 0.25,
            'tools': 0.2,
            'databases': 0.15,
            'soft_skills': 0.1
        }

    def _initialize_ai_client(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        try:
            if self.config.get('anthropic_api_key'):
                import anthropic
                return anthropic.Anthropic(api_key=self.config['anthropic_api_key'])
            elif self.config.get('openai_api_key'):
                import openai
                return openai.OpenAI(api_key=self.config['openai_api_key'])
            else:
                logger.info("æœªé…ç½®AI APIå¯†é’¥ï¼Œä½¿ç”¨åŸºç¡€åŒ¹é…ç®—æ³•")
                return None
        except ImportError as e:
            logger.error(f"AIåº“å¯¼å…¥å¤±è´¥: {e}")
            return None

    async def analyze_match(self, job_description: str, resume_content: str = "") -> str:
        """åˆ†æèŒä½åŒ¹é…åº¦

        Args:
            job_description: èŒä½æè¿°
            resume_content: ç®€å†å†…å®¹æˆ–è·¯å¾„

        Returns:
            åŒ¹é…åº¦åˆ†ææŠ¥å‘Š
        """
        try:
            # è¯»å–ç®€å†å†…å®¹
            if resume_content and Path(resume_content).exists():
                resume_content = await self._read_resume_file(resume_content)

            if self.client and resume_content:
                return await self._ai_match_analysis(job_description, resume_content)
            else:
                return await self._basic_match_analysis(job_description, resume_content)

        except Exception as e:
            logger.error(f"èŒä½åŒ¹é…åˆ†æå¤±è´¥: {e}")
            return f"åˆ†æå¤±è´¥: {str(e)}"

    async def _read_resume_file(self, file_path: str) -> str:
        """è¯»å–ç®€å†æ–‡ä»¶å†…å®¹"""
        try:
            file_path = Path(file_path)

            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()

            elif file_path.suffix.lower() == '.pdf':
                # éœ€è¦PDFè¯»å–åº“
                try:
                    import PyPDF2
                    with open(file_path, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        text = ""
                        for page in reader.pages:
                            text += page.extract_text()
                        return text
                except ImportError:
                    logger.warning("æœªå®‰è£…PyPDF2ï¼Œæ— æ³•è¯»å–PDFç®€å†")
                    return ""

            else:
                logger.warning(f"ä¸æ”¯æŒçš„ç®€å†æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return ""

        except Exception as e:
            logger.error(f"è¯»å–ç®€å†æ–‡ä»¶å¤±è´¥: {e}")
            return ""

    async def _ai_match_analysis(self, job_description: str, resume_content: str) -> str:
        """ä½¿ç”¨AIè¿›è¡Œæ·±åº¦åŒ¹é…åˆ†æ"""
        try:
            prompt = self._build_match_analysis_prompt(job_description, resume_content)

            if 'anthropic' in str(type(self.client)):
                # Claude API
                message = self.client.messages.create(
                    model=self.config.get('default_model', 'claude-3-sonnet-20240229'),
                    max_tokens=self.config.get('max_tokens', 2000),
                    temperature=self.config.get('temperature', 0.3),
                    messages=[{"role": "user", "content": prompt}]
                )
                return message.content[0].text

            elif 'openai' in str(type(self.client)):
                # OpenAI API
                response = self.client.chat.completions.create(
                    model=self.config.get('default_model', 'gpt-3.5-turbo'),
                    max_tokens=self.config.get('max_tokens', 2000),
                    temperature=self.config.get('temperature', 0.3),
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.choices[0].message.content

        except Exception as e:
            logger.error(f"AIåŒ¹é…åˆ†æå¤±è´¥: {e}")
            return await self._basic_match_analysis(job_description, resume_content)

    def _build_match_analysis_prompt(self, job_description: str, resume_content: str) -> str:
        """æ„å»ºåŒ¹é…åˆ†ææç¤º"""
        prompt = f"""
ä½œä¸ºä¸€åä¸“ä¸šçš„æ‹›è˜é¡¾é—®å’Œç®€å†åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹ç®€å†ä¸èŒä½çš„åŒ¹é…åº¦ï¼š

**èŒä½æè¿°ï¼š**
{job_description[:2000]}

**ç®€å†å†…å®¹ï¼š**
{resume_content[:2000]}

è¯·æä¾›è¯¦ç»†çš„åŒ¹é…åº¦åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

**1. æ•´ä½“åŒ¹é…åº¦è¯„åˆ† (0-100åˆ†)**
åŸºäºæŠ€èƒ½ã€ç»éªŒã€æ•™è‚²èƒŒæ™¯ç­‰å› ç´ ç»™å‡ºç»¼åˆè¯„åˆ†

**2. æŠ€èƒ½åŒ¹é…åˆ†æ**
- å®Œå…¨åŒ¹é…çš„æŠ€èƒ½
- éƒ¨åˆ†åŒ¹é…çš„æŠ€èƒ½
- ç¼ºå¤±çš„å…³é”®æŠ€èƒ½

**3. ç»éªŒåŒ¹é…åº¦**
- ç›¸å…³å·¥ä½œç»éªŒå¹´é™
- è¡Œä¸šèƒŒæ™¯å¥‘åˆåº¦
- é¡¹ç›®ç»éªŒç›¸å…³æ€§

**4. å…³é”®è¯ä¼˜åŒ–å»ºè®®**
- éœ€è¦å¢åŠ çš„å…³é”®è¯
- å¯ä»¥å¼ºåŒ–çš„æŠ€èƒ½æè¿°
- ATSç³»ç»Ÿä¼˜åŒ–å»ºè®®

**5. ç®€å†æ”¹è¿›å»ºè®®**
- å…·ä½“çš„ä¼˜åŒ–æ–¹å‘
- éœ€è¦çªå‡ºçš„ç»éªŒ
- æ ¼å¼å’Œç»“æ„å»ºè®®

**6. ç”³è¯·æˆåŠŸæ¦‚ç‡è¯„ä¼°**
åŸºäºåŒ¹é…åº¦ç»™å‡ºç”³è¯·æˆåŠŸçš„å¯èƒ½æ€§

è¯·ç”¨ç»“æ„åŒ–çš„æ ¼å¼æä¾›åˆ†æç»“æœï¼Œä½¿ç”¨emojiæ¥å¢å¼ºå¯è¯»æ€§ã€‚
"""
        return prompt

    async def _basic_match_analysis(self, job_description: str, resume_content: str = "") -> str:
        """åŸºç¡€åŒ¹é…åˆ†æï¼ˆæ— AIï¼‰"""
        try:
            # æå–èŒä½è¦æ±‚
            job_skills = self._extract_skills_from_job(job_description)
            job_keywords = self._extract_keywords(job_description)

            # åˆ†æç®€å†ï¼ˆå¦‚æœæœ‰ï¼‰
            if resume_content:
                resume_skills = self._extract_skills_from_resume(resume_content)
                resume_keywords = self._extract_keywords(resume_content)

                # è®¡ç®—åŒ¹é…åº¦
                skill_match = self._calculate_skill_match(job_skills, resume_skills)
                keyword_match = self._calculate_keyword_match(job_keywords, resume_keywords)

                overall_score = (skill_match * 0.7 + keyword_match * 0.3)

                return self._format_basic_analysis(
                    overall_score, job_skills, resume_skills, job_keywords, resume_keywords
                )
            else:
                return self._format_job_only_analysis(job_skills, job_keywords)

        except Exception as e:
            logger.error(f"åŸºç¡€åŒ¹é…åˆ†æå¤±è´¥: {e}")
            return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

    def _extract_skills_from_job(self, job_description: str) -> Dict[str, List[str]]:
        """ä»èŒä½æè¿°ä¸­æå–æŠ€èƒ½"""
        skills = {
            'programming_languages': [],
            'frameworks': [],
            'tools': [],
            'databases': [],
            'soft_skills': []
        }

        # ç¼–ç¨‹è¯­è¨€
        prog_langs = ['Python', 'JavaScript', 'Java', 'C++', 'C#', 'Go', 'Rust', 'PHP', 'Ruby', 'Scala', 'Kotlin', 'Swift', 'TypeScript']
        # æ¡†æ¶
        frameworks = ['React', 'Vue', 'Angular', 'Django', 'Flask', 'Spring', 'Express', 'Laravel', 'Rails', 'ASP.NET']
        # å·¥å…·
        tools = ['Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'Git', 'Jenkins', 'Terraform', 'Ansible']
        # æ•°æ®åº“
        databases = ['MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Elasticsearch', 'Oracle', 'SQL Server']
        # è½¯æŠ€èƒ½
        soft_skills = ['leadership', 'communication', 'teamwork', 'problem solving', 'analytical', 'creative']

        job_desc_lower = job_description.lower()

        for lang in prog_langs:
            if lang.lower() in job_desc_lower:
                skills['programming_languages'].append(lang)

        for framework in frameworks:
            if framework.lower() in job_desc_lower:
                skills['frameworks'].append(framework)

        for tool in tools:
            if tool.lower() in job_desc_lower:
                skills['tools'].append(tool)

        for db in databases:
            if db.lower() in job_desc_lower:
                skills['databases'].append(db)

        for skill in soft_skills:
            if skill in job_desc_lower:
                skills['soft_skills'].append(skill)

        return skills

    def _extract_skills_from_resume(self, resume_content: str) -> Dict[str, List[str]]:
        """ä»ç®€å†ä¸­æå–æŠ€èƒ½"""
        # å¤ç”¨èŒä½æŠ€èƒ½æå–é€»è¾‘
        return self._extract_skills_from_job(resume_content)

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶è½¬æ¢ä¸ºå°å†™
        cleaned_text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = cleaned_text.split()

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were'}
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]

        # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
        from collections import Counter
        word_counts = Counter(keywords)
        return [word for word, count in word_counts.most_common(20)]

    def _calculate_skill_match(self, job_skills: Dict, resume_skills: Dict) -> float:
        """è®¡ç®—æŠ€èƒ½åŒ¹é…åº¦"""
        total_score = 0.0
        total_weight = 0.0

        for category, weight in self.skill_weights.items():
            job_category_skills = set(skill.lower() for skill in job_skills.get(category, []))
            resume_category_skills = set(skill.lower() for skill in resume_skills.get(category, []))

            if job_category_skills:
                match_count = len(job_category_skills.intersection(resume_category_skills))
                category_score = match_count / len(job_category_skills)
                total_score += category_score * weight
                total_weight += weight

        return (total_score / total_weight * 100) if total_weight > 0 else 0.0

    def _calculate_keyword_match(self, job_keywords: List[str], resume_keywords: List[str]) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åº¦"""
        if not job_keywords:
            return 0.0

        job_set = set(job_keywords)
        resume_set = set(resume_keywords)

        match_count = len(job_set.intersection(resume_set))
        return (match_count / len(job_set)) * 100

    def _format_basic_analysis(
        self,
        overall_score: float,
        job_skills: Dict,
        resume_skills: Dict,
        job_keywords: List[str],
        resume_keywords: List[str]
    ) -> str:
        """æ ¼å¼åŒ–åŸºç¡€åˆ†æç»“æœ"""
        # è®¡ç®—åŒ¹é…å’Œç¼ºå¤±çš„æŠ€èƒ½
        matched_skills = []
        missing_skills = []

        for category in job_skills:
            job_category_skills = set(skill.lower() for skill in job_skills[category])
            resume_category_skills = set(skill.lower() for skill in resume_skills[category])

            matched = job_category_skills.intersection(resume_category_skills)
            missing = job_category_skills - resume_category_skills

            matched_skills.extend([skill.title() for skill in matched])
            missing_skills.extend([skill.title() for skill in missing])

        # æ ¼å¼åŒ–ç»“æœ
        result = f"""# ğŸ“Š èŒä½åŒ¹é…åº¦åˆ†ææŠ¥å‘Š

## ğŸ¯ æ•´ä½“åŒ¹é…åº¦è¯„åˆ†
**{overall_score:.1f}/100åˆ†** {'ğŸŸ¢' if overall_score >= 70 else 'ğŸŸ¡' if overall_score >= 50 else 'ğŸ”´'}

## âœ… æŠ€èƒ½åŒ¹é…æƒ…å†µ

### åŒ¹é…çš„æŠ€èƒ½ ({len(matched_skills)} é¡¹)
{', '.join(matched_skills) if matched_skills else 'æš‚æœªå‘ç°æ˜æ˜¾åŒ¹é…çš„æŠ€èƒ½'}

### âš ï¸ ç¼ºå¤±çš„å…³é”®æŠ€èƒ½ ({len(missing_skills)} é¡¹)
{', '.join(missing_skills) if missing_skills else 'æ‰€æœ‰å…³é”®æŠ€èƒ½éƒ½å·²è¦†ç›–'}

## ğŸ” å…³é”®è¯åˆ†æ

### èŒä½å…³é”®è¯ (Top 10)
{', '.join(job_keywords[:10])}

### ç®€å†å…³é”®è¯è¦†ç›–
{', '.join(set(job_keywords).intersection(set(resume_keywords))) if set(job_keywords).intersection(set(resume_keywords)) else 'å…³é”®è¯è¦†ç›–è¾ƒå°‘'}

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

### ç«‹å³æ”¹è¿›é¡¹
"""

        if overall_score < 50:
            result += """
- ğŸ”´ **åŒ¹é…åº¦è¾ƒä½**ï¼šå»ºè®®é‡æ–°å®¡è§†æ˜¯å¦é€‚åˆç”³è¯·æ­¤èŒä½
- ğŸ“š **æŠ€èƒ½æå‡**ï¼šä¼˜å…ˆå­¦ä¹ ç¼ºå¤±çš„å…³é”®æŠ€èƒ½
- ğŸ“ **ç®€å†ä¼˜åŒ–**ï¼šå¢åŠ ç›¸å…³é¡¹ç›®ç»éªŒæè¿°
"""
        elif overall_score < 70:
            result += """
- ğŸŸ¡ **æœ‰ä¸€å®šæœºä¼š**ï¼šå¯ä»¥å°è¯•ç”³è¯·ï¼Œä½†éœ€è¦ä¼˜åŒ–
- ğŸ¯ **çªå‡ºä¼˜åŠ¿**ï¼šåœ¨æ±‚èŒä¿¡ä¸­å¼ºè°ƒåŒ¹é…çš„æŠ€èƒ½
- ğŸ“ˆ **è¡¥å……ç»éªŒ**ï¼šå¯»æ‰¾ç›¸å…³é¡¹ç›®æˆ–åŸ¹è®­ç»å†
"""
        else:
            result += """
- ğŸŸ¢ **åŒ¹é…åº¦è‰¯å¥½**ï¼šå»ºè®®ç§¯æç”³è¯·
- â­ **ä¿æŒä¼˜åŠ¿**ï¼šç»§ç»­å¼ºåŒ–å·²æœ‰æŠ€èƒ½
- ğŸš€ **å±•ç¤ºæˆæœ**ï¼šçªå‡ºç›¸å…³é¡¹ç›®çš„å…·ä½“æˆæœ
"""

        if missing_skills:
            result += f"\n### ğŸ¯ æŠ€èƒ½å‘å±•é‡ç‚¹\n"
            for skill in missing_skills[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæœ€é‡è¦çš„ç¼ºå¤±æŠ€èƒ½
                result += f"- å­¦ä¹ å’ŒæŒæ¡ **{skill}**\n"

        return result

    def _format_job_only_analysis(self, job_skills: Dict, job_keywords: List[str]) -> str:
        """æ ¼å¼åŒ–ä»…èŒä½åˆ†æç»“æœ"""
        total_skills = sum(len(skills) for skills in job_skills.values())

        result = f"""# ğŸ“‹ èŒä½è¦æ±‚åˆ†æ

## ğŸ¯ æŠ€èƒ½è¦æ±‚æ¦‚è§ˆ (å…± {total_skills} é¡¹æŠ€èƒ½)

"""

        for category, skills in job_skills.items():
            if skills:
                category_name = {
                    'programming_languages': 'ç¼–ç¨‹è¯­è¨€',
                    'frameworks': 'æ¡†æ¶/åº“',
                    'tools': 'å·¥å…·/å¹³å°',
                    'databases': 'æ•°æ®åº“',
                    'soft_skills': 'è½¯æŠ€èƒ½'
                }.get(category, category)

                result += f"### {category_name}\n"
                result += f"{', '.join(skills)}\n\n"

        result += f"""## ğŸ”‘ å…³é”®è¯é‡ç‚¹

### é«˜é¢‘å…³é”®è¯ (Top 10)
{', '.join(job_keywords[:10])}

## ğŸ’¡ ç”³è¯·å»ºè®®

### ç®€å†ä¼˜åŒ–é‡ç‚¹
- âœ… ç¡®ä¿ç®€å†åŒ…å«ä¸Šè¿°å…³é”®æŠ€èƒ½
- ğŸ“ ä½¿ç”¨ç›¸åŒçš„æŠ€æœ¯æœ¯è¯­å’Œå…³é”®è¯
- ğŸ¯ çªå‡ºç›¸å…³é¡¹ç›®ç»éªŒå’Œæˆæœ
- ğŸ“Š é‡åŒ–å·¥ä½œæˆæœå’Œå½±å“

### æ±‚èŒä¿¡è¦ç‚¹
- ğŸ¨ é’ˆå¯¹æ€§åœ°å±•ç¤ºåŒ¹é…çš„æŠ€èƒ½
- ğŸ’¼ å¼ºè°ƒç›¸å…³è¡Œä¸šç»éªŒ
- ğŸš€ ä½“ç°å­¦ä¹ èƒ½åŠ›å’Œé€‚åº”æ€§
- ğŸ¤ å±•ç°å›¢é˜Ÿåˆä½œå’Œæ²Ÿé€šèƒ½åŠ›

## âš ï¸ æ³¨æ„äº‹é¡¹
- å¦‚æœç¼ºå°‘å…³é”®æŠ€èƒ½ï¼Œè€ƒè™‘å…ˆè¿›è¡Œå­¦ä¹ æˆ–åŸ¹è®­
- å‡†å¤‡é¢è¯•æ—¶é‡ç‚¹å¤ä¹ ç›¸å…³æŠ€æœ¯çŸ¥è¯†
- äº†è§£å…¬å¸èƒŒæ™¯å’Œè¡Œä¸šè¶‹åŠ¿
"""

        return result

if __name__ == "__main__":
    async def test_job_matcher():
        """æµ‹è¯•èŒä½åŒ¹é…å™¨"""
        config = {
            'anthropic_api_key': '',  # éœ€è¦å®é™…çš„APIå¯†é’¥
            'default_model': 'claude-3-sonnet-20240229',
            'max_tokens': 2000,
            'temperature': 0.3
        }

        matcher = JobMatcher(config)

        job_desc = """
        Senior Python Developer position requiring:
        - 5+ years Python development experience
        - Django, Flask frameworks
        - REST API development
        - AWS cloud platforms
        - Docker, Kubernetes
        - PostgreSQL, Redis
        - Strong problem-solving skills
        """

        resume = """
        Senior Software Developer with 6 years experience in Python development.
        Expert in Django and Flask frameworks. Built scalable REST APIs.
        Experience with AWS services including EC2, S3, RDS.
        Proficient in Docker containerization and Kubernetes orchestration.
        Database experience with PostgreSQL and MongoDB.
        Strong analytical and communication skills.
        """

        analysis = await matcher.analyze_match(job_desc, resume)
        print(analysis)

    asyncio.run(test_job_matcher())